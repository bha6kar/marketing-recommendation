{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full pipeline\n",
    "\n",
    "Pipeline includes preprocess the data, feature extraction, train model, evaluate model and predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path_id</th>\n",
       "      <th>device_type</th>\n",
       "      <th>attribution_channel</th>\n",
       "      <th>distance_to_last</th>\n",
       "      <th>time_delta_in_days</th>\n",
       "      <th>has_booked</th>\n",
       "      <th>n_sessions</th>\n",
       "      <th>most_common_landing_page</th>\n",
       "      <th>clicked_city</th>\n",
       "      <th>viewed_city</th>\n",
       "      <th>viewed_beach</th>\n",
       "      <th>saw_brand</th>\n",
       "      <th>saw_organic</th>\n",
       "      <th>saw_direct</th>\n",
       "      <th>saw_offer_summary</th>\n",
       "      <th>saw_panda</th>\n",
       "      <th>adults</th>\n",
       "      <th>children</th>\n",
       "      <th>nights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4c56d801d41290b7b204b55e1565689f</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Search</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4c56d801d41290b7b204b55e1565689f</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Search</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4c56d801d41290b7b204b55e1565689f</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Search</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4c56d801d41290b7b204b55e1565689f</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Search</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4c56d801d41290b7b204b55e1565689f</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Search</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            path_id  device_type  attribution_channel  \\\n",
       "0  4c56d801d41290b7b204b55e1565689f            2                    1   \n",
       "1  4c56d801d41290b7b204b55e1565689f            2                    9   \n",
       "2  4c56d801d41290b7b204b55e1565689f            2                    9   \n",
       "3  4c56d801d41290b7b204b55e1565689f            2                    9   \n",
       "4  4c56d801d41290b7b204b55e1565689f            2                    9   \n",
       "\n",
       "   distance_to_last  time_delta_in_days  has_booked  n_sessions  \\\n",
       "0                 1                   0           1           6   \n",
       "1                 6                   1           1           6   \n",
       "2                 4                   0           1           6   \n",
       "3                 2                   0           1           6   \n",
       "4                 7                   1           1           6   \n",
       "\n",
       "  most_common_landing_page clicked_city viewed_city viewed_beach saw_brand  \\\n",
       "0                   Search        False       False        False     False   \n",
       "1                   Search        False       False        False     False   \n",
       "2                   Search        False       False        False     False   \n",
       "3                   Search        False       False        False     False   \n",
       "4                   Search        False       False        False     False   \n",
       "\n",
       "  saw_organic saw_direct  saw_offer_summary  saw_panda  adults  children  \\\n",
       "0       False       True               True       True     6.0  0.333333   \n",
       "1       False       True               True       True     6.0  0.333333   \n",
       "2       False       True               True       True     6.0  0.333333   \n",
       "3       False       True               True       True     6.0  0.333333   \n",
       "4       False       True               True       True     6.0  0.333333   \n",
       "\n",
       "     nights  \n",
       "0  0.666667  \n",
       "1  0.666667  \n",
       "2  0.666667  \n",
       "3  0.666667  \n",
       "4  0.666667  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from src.main.preprocess_data import merge_data\n",
    "\n",
    "merged_data: DataFrame = merge_data(\n",
    "    \"data/raw/attribution_path_data.feather\", \"data/raw/user_feature_data.feather\"\n",
    ")\n",
    "merged_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device_type</th>\n",
       "      <th>attribution_channel</th>\n",
       "      <th>distance_to_last</th>\n",
       "      <th>time_delta_in_days</th>\n",
       "      <th>has_booked</th>\n",
       "      <th>n_sessions</th>\n",
       "      <th>most_common_landing_page</th>\n",
       "      <th>clicked_city</th>\n",
       "      <th>viewed_city</th>\n",
       "      <th>viewed_beach</th>\n",
       "      <th>saw_brand</th>\n",
       "      <th>saw_organic</th>\n",
       "      <th>saw_direct</th>\n",
       "      <th>saw_offer_summary</th>\n",
       "      <th>saw_panda</th>\n",
       "      <th>adults</th>\n",
       "      <th>children</th>\n",
       "      <th>nights</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>path_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00003d039958362817073f4c9448ff34</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Search</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00004fcd09bcf6bccb16269538032578</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>SEO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000fc42234be9ec2abcbb4c71b4bae4</th>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Homepage</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000148e9e616d1a4f03c92f566f37b39</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel Details</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00014d81cd6599192b2446c4c65a29c0</th>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel Details</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffeb83084cd63bf98f06a9831713ade</th>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>SEO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffecfee29bb3142a438092ee6ffd638</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Homepage</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffedd1c982de9688cc6bd78a8a8a399</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>SEO</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffff36f1ceada40b90f268dd590460ad</th>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Homepage</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffffb7b95695335c8199e5610a31bcaf</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Homepage</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>228101 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  device_type  attribution_channel  \\\n",
       "path_id                                                              \n",
       "00003d039958362817073f4c9448ff34            3                    7   \n",
       "00004fcd09bcf6bccb16269538032578            3                    4   \n",
       "0000fc42234be9ec2abcbb4c71b4bae4            3                   17   \n",
       "000148e9e616d1a4f03c92f566f37b39            3                    7   \n",
       "00014d81cd6599192b2446c4c65a29c0            5                   21   \n",
       "...                                       ...                  ...   \n",
       "fffeb83084cd63bf98f06a9831713ade            5                   21   \n",
       "fffecfee29bb3142a438092ee6ffd638            2                    2   \n",
       "fffedd1c982de9688cc6bd78a8a8a399            3                    9   \n",
       "ffff36f1ceada40b90f268dd590460ad            3                   17   \n",
       "ffffb7b95695335c8199e5610a31bcaf            3                    9   \n",
       "\n",
       "                                  distance_to_last  time_delta_in_days  \\\n",
       "path_id                                                                  \n",
       "00003d039958362817073f4c9448ff34                 2                   1   \n",
       "00004fcd09bcf6bccb16269538032578                 2                   2   \n",
       "0000fc42234be9ec2abcbb4c71b4bae4                 3                   1   \n",
       "000148e9e616d1a4f03c92f566f37b39                 2                  19   \n",
       "00014d81cd6599192b2446c4c65a29c0                 2                   0   \n",
       "...                                            ...                 ...   \n",
       "fffeb83084cd63bf98f06a9831713ade                 2                   0   \n",
       "fffecfee29bb3142a438092ee6ffd638                 4                   4   \n",
       "fffedd1c982de9688cc6bd78a8a8a399                 2                   0   \n",
       "ffff36f1ceada40b90f268dd590460ad                 2                   0   \n",
       "ffffb7b95695335c8199e5610a31bcaf                 3                   0   \n",
       "\n",
       "                                  has_booked  n_sessions  \\\n",
       "path_id                                                    \n",
       "00003d039958362817073f4c9448ff34           0           1   \n",
       "00004fcd09bcf6bccb16269538032578           0           1   \n",
       "0000fc42234be9ec2abcbb4c71b4bae4           0           2   \n",
       "000148e9e616d1a4f03c92f566f37b39           0           1   \n",
       "00014d81cd6599192b2446c4c65a29c0           0           1   \n",
       "...                                      ...         ...   \n",
       "fffeb83084cd63bf98f06a9831713ade           0           1   \n",
       "fffecfee29bb3142a438092ee6ffd638           1           3   \n",
       "fffedd1c982de9688cc6bd78a8a8a399           0           1   \n",
       "ffff36f1ceada40b90f268dd590460ad           0           1   \n",
       "ffffb7b95695335c8199e5610a31bcaf           0           2   \n",
       "\n",
       "                                 most_common_landing_page clicked_city  \\\n",
       "path_id                                                                  \n",
       "00003d039958362817073f4c9448ff34                   Search        False   \n",
       "00004fcd09bcf6bccb16269538032578                      SEO          NaN   \n",
       "0000fc42234be9ec2abcbb4c71b4bae4                 Homepage        False   \n",
       "000148e9e616d1a4f03c92f566f37b39            Hotel Details          NaN   \n",
       "00014d81cd6599192b2446c4c65a29c0            Hotel Details          NaN   \n",
       "...                                                   ...          ...   \n",
       "fffeb83084cd63bf98f06a9831713ade                      SEO          NaN   \n",
       "fffecfee29bb3142a438092ee6ffd638                 Homepage        False   \n",
       "fffedd1c982de9688cc6bd78a8a8a399                      SEO        False   \n",
       "ffff36f1ceada40b90f268dd590460ad                 Homepage        False   \n",
       "ffffb7b95695335c8199e5610a31bcaf                 Homepage        False   \n",
       "\n",
       "                                 viewed_city viewed_beach saw_brand  \\\n",
       "path_id                                                               \n",
       "00003d039958362817073f4c9448ff34       False        False     False   \n",
       "00004fcd09bcf6bccb16269538032578         NaN          NaN     False   \n",
       "0000fc42234be9ec2abcbb4c71b4bae4       False        False      True   \n",
       "000148e9e616d1a4f03c92f566f37b39       False        False     False   \n",
       "00014d81cd6599192b2446c4c65a29c0       False        False     False   \n",
       "...                                      ...          ...       ...   \n",
       "fffeb83084cd63bf98f06a9831713ade         NaN          NaN     False   \n",
       "fffecfee29bb3142a438092ee6ffd638       False        False      True   \n",
       "fffedd1c982de9688cc6bd78a8a8a399         NaN          NaN     False   \n",
       "ffff36f1ceada40b90f268dd590460ad       False        False      True   \n",
       "ffffb7b95695335c8199e5610a31bcaf       False        False     False   \n",
       "\n",
       "                                 saw_organic saw_direct  saw_offer_summary  \\\n",
       "path_id                                                                      \n",
       "00003d039958362817073f4c9448ff34       False      False              False   \n",
       "00004fcd09bcf6bccb16269538032578       False      False              False   \n",
       "0000fc42234be9ec2abcbb4c71b4bae4       False      False               True   \n",
       "000148e9e616d1a4f03c92f566f37b39       False      False              False   \n",
       "00014d81cd6599192b2446c4c65a29c0       False      False              False   \n",
       "...                                      ...        ...                ...   \n",
       "fffeb83084cd63bf98f06a9831713ade        True      False              False   \n",
       "fffecfee29bb3142a438092ee6ffd638       False      False               True   \n",
       "fffedd1c982de9688cc6bd78a8a8a399       False      False              False   \n",
       "ffff36f1ceada40b90f268dd590460ad       False      False              False   \n",
       "ffffb7b95695335c8199e5610a31bcaf       False       True               True   \n",
       "\n",
       "                                  saw_panda  adults  children  nights  \n",
       "path_id                                                                \n",
       "00003d039958362817073f4c9448ff34       True     6.0       0.0     1.0  \n",
       "00004fcd09bcf6bccb16269538032578      False     0.0       0.0     0.0  \n",
       "0000fc42234be9ec2abcbb4c71b4bae4       True     1.0       0.0     1.0  \n",
       "000148e9e616d1a4f03c92f566f37b39       True     2.0       0.0     1.0  \n",
       "00014d81cd6599192b2446c4c65a29c0      False     0.0       0.0     1.0  \n",
       "...                                     ...     ...       ...     ...  \n",
       "fffeb83084cd63bf98f06a9831713ade      False     0.0       0.0     0.0  \n",
       "fffecfee29bb3142a438092ee6ffd638       True     2.0       0.0     1.0  \n",
       "fffedd1c982de9688cc6bd78a8a8a399      False     0.0       0.0     0.0  \n",
       "ffff36f1ceada40b90f268dd590460ad       True     2.0       1.0     1.0  \n",
       "ffffb7b95695335c8199e5610a31bcaf       True     2.0       0.0     1.0  \n",
       "\n",
       "[228101 rows x 18 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group the data by path ID and aggregate the features\n",
    "grouped_data: DataFrame = merged_data.groupby(\"path_id\").agg(\n",
    "    {\n",
    "        \"device_type\": lambda x: x.mode().iloc[0],\n",
    "        \"attribution_channel\": lambda x: x.mode().iloc[0],\n",
    "        \"distance_to_last\": \"max\",\n",
    "        \"time_delta_in_days\": \"max\",\n",
    "        \"has_booked\": \"max\",\n",
    "        \"n_sessions\": \"max\",\n",
    "        \"most_common_landing_page\": lambda x: x.mode().iloc[0],\n",
    "        \"clicked_city\": \"max\",\n",
    "        \"viewed_city\": \"max\",\n",
    "        \"viewed_beach\": \"max\",\n",
    "        \"saw_brand\": \"max\",\n",
    "        \"saw_organic\": \"max\",\n",
    "        \"saw_direct\": \"max\",\n",
    "        \"saw_offer_summary\": \"max\",\n",
    "        \"saw_panda\": \"max\",\n",
    "        \"adults\": \"max\",\n",
    "        \"children\": \"max\",\n",
    "        \"nights\": \"max\",\n",
    "    }\n",
    ")\n",
    "grouped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting some data for prediction purpose.\n",
    "\n",
    "Splitting data for prediction purpose in last step as use this data as new data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from pandas import DataFrame, concat\n",
    "\n",
    "X: DataFrame = grouped_data.drop(\"has_booked\", axis=1)\n",
    "y = grouped_data[\"has_booked\"].astype(int)\n",
    "X_train, X_for_pred, y_train, y_for_pred = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Combine X and y back into a single DataFrame\n",
    "combined_df: DataFrame = concat([X_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data\n",
    "\n",
    "Preprocess data by cleaning, transforming categorical features to numerical values and imputing NA values.\n",
    "\n",
    "#### Note:\n",
    "\n",
    "    - The function modifies the input DataFrame in place and return processed DataFrame.\n",
    "    - If save_path is provided, the preprocessed data will be saved as a Feather file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Preprocessing data -----------\n",
      "Dropping unnecessary features: ['clicked_city', 'viewed_city', 'viewed_beach']\n"
     ]
    }
   ],
   "source": [
    "from src.main.preprocess_data import preprocess_data\n",
    "\n",
    "processed_data: DataFrame = preprocess_data(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "#### Note:\n",
    "\n",
    "    - Adjust the list of features to drop, based on the specific features needed for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Feature selection -----------\n",
      "Dropping unnecessary features: ['time_delta_in_days', 'distance_to_last']\n"
     ]
    }
   ],
   "source": [
    "from src.main.feature_selection import feature_selection, train_val_test_split\n",
    "\n",
    "featured_data: DataFrame = feature_selection(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 182480 entries, 0 to 182479\n",
      "Data columns (total 14 columns):\n",
      " #   Column                    Non-Null Count   Dtype  \n",
      "---  ------                    --------------   -----  \n",
      " 0   device_type               182480 non-null  float64\n",
      " 1   attribution_channel       182480 non-null  float64\n",
      " 2   n_sessions                182480 non-null  float64\n",
      " 3   most_common_landing_page  182480 non-null  float64\n",
      " 4   saw_brand                 182480 non-null  float64\n",
      " 5   saw_organic               182480 non-null  float64\n",
      " 6   saw_direct                182480 non-null  float64\n",
      " 7   saw_offer_summary         182480 non-null  float64\n",
      " 8   saw_panda                 182480 non-null  float64\n",
      " 9   adults                    182480 non-null  float64\n",
      " 10  children                  182480 non-null  float64\n",
      " 11  nights                    182480 non-null  float64\n",
      " 12  has_booked                182480 non-null  float64\n",
      " 13  time_to_book_week         182480 non-null  float64\n",
      "dtypes: float64(14)\n",
      "memory usage: 19.5 MB\n"
     ]
    }
   ],
   "source": [
    "featured_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device_type</th>\n",
       "      <th>attribution_channel</th>\n",
       "      <th>n_sessions</th>\n",
       "      <th>most_common_landing_page</th>\n",
       "      <th>saw_brand</th>\n",
       "      <th>saw_organic</th>\n",
       "      <th>saw_direct</th>\n",
       "      <th>saw_offer_summary</th>\n",
       "      <th>saw_panda</th>\n",
       "      <th>adults</th>\n",
       "      <th>children</th>\n",
       "      <th>nights</th>\n",
       "      <th>has_booked</th>\n",
       "      <th>time_to_book_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.048148</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.855556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.06250</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182475</th>\n",
       "      <td>3.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182476</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182477</th>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182478</th>\n",
       "      <td>3.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.03125</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.122222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182479</th>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.344444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>182480 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        device_type  attribution_channel  n_sessions  \\\n",
       "0               3.0                  9.0         1.0   \n",
       "1               3.0                 20.0         3.0   \n",
       "2               3.0                 18.0        36.0   \n",
       "3               3.0                  9.0         2.0   \n",
       "4               3.0                 21.0         6.0   \n",
       "...             ...                  ...         ...   \n",
       "182475          3.0                 31.0         1.0   \n",
       "182476          2.0                  9.0         4.0   \n",
       "182477          3.0                  9.0         8.0   \n",
       "182478          3.0                 20.0         4.0   \n",
       "182479          3.0                  9.0         7.0   \n",
       "\n",
       "        most_common_landing_page  saw_brand  saw_organic  saw_direct  \\\n",
       "0                           10.0        0.0          0.0         1.0   \n",
       "1                           17.0        0.0          0.0         0.0   \n",
       "2                           17.0        1.0          1.0         1.0   \n",
       "3                           12.0        0.0          0.0         1.0   \n",
       "4                           11.0        1.0          0.0         0.0   \n",
       "...                          ...        ...          ...         ...   \n",
       "182475                      19.0        0.0          1.0         0.0   \n",
       "182476                      12.0        0.0          0.0         1.0   \n",
       "182477                      10.0        0.0          0.0         1.0   \n",
       "182478                      17.0        0.0          1.0         0.0   \n",
       "182479                      16.0        0.0          0.0         1.0   \n",
       "\n",
       "        saw_offer_summary  saw_panda    adults  children    nights  \\\n",
       "0                     0.0        0.0  0.000000   0.00000  0.000000   \n",
       "1                     0.0        1.0  0.088889   0.00000  0.666667   \n",
       "2                     1.0        1.0  0.048148   0.00000  0.555556   \n",
       "3                     0.0        0.0  0.000000   0.00000  0.000000   \n",
       "4                     1.0        1.0  0.088889   0.06250  1.000000   \n",
       "...                   ...        ...       ...       ...       ...   \n",
       "182475                0.0        0.0  0.000000   0.00000  0.000000   \n",
       "182476                0.0        0.0  0.000000   0.00000  0.000000   \n",
       "182477                0.0        0.0  0.000000   0.00000  0.000000   \n",
       "182478                0.0        1.0  0.033333   0.03125  0.250000   \n",
       "182479                0.0        1.0  0.095238   0.00000  0.714286   \n",
       "\n",
       "        has_booked  time_to_book_week  \n",
       "0              0.0           0.000000  \n",
       "1              0.0           0.266667  \n",
       "2              1.0           0.855556  \n",
       "3              0.0           0.500000  \n",
       "4              0.0           0.011111  \n",
       "...            ...                ...  \n",
       "182475         0.0           0.000000  \n",
       "182476         0.0           0.088889  \n",
       "182477         0.0           0.833333  \n",
       "182478         0.0           0.122222  \n",
       "182479         0.0           0.344444  \n",
       "\n",
       "[182480 rows x 14 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featured_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting data for training, validation, and testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_val, y_train, y_test, y_val = train_val_test_split(featured_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "Train the model and save the model.\n",
    "\n",
    "#### Note:\n",
    "\n",
    "    - The function uses the following models: Logistic Regression, Random Forest, XGBoost, LSTM.\n",
    "    - As this is classification task so those basic classification models are chosen for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression with GridSearchCV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest Classifier with GridSearchCV...\n",
      "Training XGBoost with GridSearchCV...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter num_units for estimator KerasClassifier.\nThis issue can likely be resolved by setting this parameter in the KerasClassifier constructor:\n`KerasClassifier(num_units=32)`\nCheck the list of available parameters with `estimator.get_params().keys()`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/bhaskar.saikia/Downloads/Senior Data Scientist Task/full_pipeline.ipynb Cell 17\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bhaskar.saikia/Downloads/Senior%20Data%20Scientist%20Task/full_pipeline.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrain_model_optimized\u001b[39;00m \u001b[39mimport\u001b[39;00m train_models\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/bhaskar.saikia/Downloads/Senior%20Data%20Scientist%20Task/full_pipeline.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_models(X_train, y_train)\n",
      "File \u001b[0;32m~/Downloads/Senior Data Scientist Task/src/main/train_model_optimized.py:248\u001b[0m, in \u001b[0;36mtrain_models\u001b[0;34m(X_train, y_train)\u001b[0m\n\u001b[1;32m    246\u001b[0m get_model_RandomForestClassifier(X_train, y_train)\n\u001b[1;32m    247\u001b[0m get_model_XGBoost(X_train, y_train)\n\u001b[0;32m--> 248\u001b[0m get_model_LSTM(X_train, y_train)\n",
      "File \u001b[0;32m~/Downloads/Senior Data Scientist Task/src/main/train_model_optimized.py:174\u001b[0m, in \u001b[0;36mget_model_LSTM\u001b[0;34m(X_train, y_train, save_path)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[39m# Perform grid search\u001b[39;00m\n\u001b[1;32m    173\u001b[0m grid \u001b[39m=\u001b[39m GridSearchCV(estimator\u001b[39m=\u001b[39mmodel, param_grid\u001b[39m=\u001b[39mparam_grid, scoring\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m\"\u001b[39m, cv\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m--> 174\u001b[0m grid_result \u001b[39m=\u001b[39m grid\u001b[39m.\u001b[39;49mfit(X_train_3d, y_train, class_weight\u001b[39m=\u001b[39;49mclass_weight_dict)\n\u001b[1;32m    176\u001b[0m \u001b[39m# Get the best parameters from the grid search\u001b[39;00m\n\u001b[1;32m    177\u001b[0m best_params \u001b[39m=\u001b[39m grid_result\u001b[39m.\u001b[39mbest_params_\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    896\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    900\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    902\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1421\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1422\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/model_selection/_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    838\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    839\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    842\u001b[0m         )\n\u001b[1;32m    843\u001b[0m     )\n\u001b[0;32m--> 845\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    847\u001b[0m         clone(base_estimator),\n\u001b[1;32m    848\u001b[0m         X,\n\u001b[1;32m    849\u001b[0m         y,\n\u001b[1;32m    850\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    851\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    852\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    853\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    854\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    855\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    856\u001b[0m     )\n\u001b[1;32m    857\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    858\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    859\u001b[0m     )\n\u001b[1;32m    860\u001b[0m )\n\u001b[1;32m    862\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    863\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    865\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    866\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n\u001b[1;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:717\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    710\u001b[0m fit_params \u001b[39m=\u001b[39m _check_fit_params(X, fit_params, train)\n\u001b[1;32m    712\u001b[0m \u001b[39mif\u001b[39;00m parameters \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    713\u001b[0m     \u001b[39m# here we clone the parameters, since sometimes the parameters\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[39m# themselves might be estimators, e.g. when we search over different\u001b[39;00m\n\u001b[1;32m    715\u001b[0m     \u001b[39m# estimators in a pipeline.\u001b[39;00m\n\u001b[1;32m    716\u001b[0m     \u001b[39m# ref: https://github.com/scikit-learn/scikit-learn/pull/26786\u001b[39;00m\n\u001b[0;32m--> 717\u001b[0m     estimator \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39;49mset_params(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mclone(parameters, safe\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m))\n\u001b[1;32m    719\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    721\u001b[0m X_train, y_train \u001b[39m=\u001b[39m _safe_split(estimator, X, y, train)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/scikeras/wrappers.py:1165\u001b[0m, in \u001b[0;36mBaseWrapper.set_params\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m   1161\u001b[0m             \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mset_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{param: value})\n\u001b[1;32m   1162\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m   1163\u001b[0m             \u001b[39m# Give a SciKeras specific user message to aid\u001b[39;00m\n\u001b[1;32m   1164\u001b[0m             \u001b[39m# in moving from the Keras wrappers\u001b[39;00m\n\u001b[0;32m-> 1165\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1166\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid parameter \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m for estimator \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1167\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mThis issue can likely be resolved by setting this parameter\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1168\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m in the \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m constructor:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1169\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m)`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1170\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mCheck the list of available parameters with\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1171\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m `estimator.get_params().keys()`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1172\u001b[0m             ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1173\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid parameter num_units for estimator KerasClassifier.\nThis issue can likely be resolved by setting this parameter in the KerasClassifier constructor:\n`KerasClassifier(num_units=32)`\nCheck the list of available parameters with `estimator.get_params().keys()`"
     ]
    }
   ],
   "source": [
    "from src.main.train_model_optimized import train_models\n",
    "\n",
    "train_models(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model\n",
    "\n",
    "Loads the model as per its extension.\n",
    "\n",
    "#### Note:\n",
    "\n",
    "    - Supports loading models saved with joblib (.joblib), pickle (.pkl), and Keras (.keras) formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.main.load_model import load_model_from_file\n",
    "\n",
    "models = [\n",
    "    (\"RandomForest\", \"models/model_rf.joblib\"),\n",
    "    (\"LogisticRegression\", \"models/model_lr.joblib\"),\n",
    "    (\"XGBoost\", \"models/model_xgboost.pkl\"),\n",
    "    (\"LSTM\", \"models/model_lstm.keras\"),\n",
    "]\n",
    "\n",
    "# Load models into a list\n",
    "loaded_models = [load_model_from_file(file_path) for _, file_path in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loaded_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "\n",
    "#### Note:\n",
    "\n",
    "    - Uses binary classification for the evaluation metrics.\n",
    "    - Uses specificity scores for model comparison.\n",
    "    - Returns the best model based on the highest specificity score.\n",
    "\n",
    "SpecificityScore <- Because it will return correctly the true number of people who will not book and we can target them with the marketing strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.main.evaluate_model import model_selection, get_scores\n",
    "\n",
    "metric = \"f1\"\n",
    "model = model_selection(loaded_models, X_val, y_val, metric)\n",
    "\n",
    "print(\n",
    "    f\"\\n The best model suits the data by highest {metric} metrics is: {model.__class__.__name__}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix for Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.heatmap(\n",
    "    conf_matrix,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    cbar=False,\n",
    "    xticklabels=[\"Predicted 0\", \"Predicted 1\"],\n",
    "    yticklabels=[\"Actual 0\", \"Actual 1\"],\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred, normalize=\"all\")\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.heatmap(\n",
    "    conf_matrix,\n",
    "    annot=True,\n",
    "    cmap=\"Blues\",\n",
    "    fmt=\".2%\",\n",
    "    cbar=False,\n",
    "    xticklabels=[\"Predicted 0\", \"Predicted 1\"],\n",
    "    yticklabels=[\"Actual 0\", \"Actual 1\"],\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix (Percentage)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "feature_names = X_train.columns.to_list()\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "\n",
    "feature_names_sorted = np.array(feature_names)[np.argsort(feature_importances)[::-1]]\n",
    "feature_importances_sorted = np.sort(feature_importances)[::-1]\n",
    "\n",
    "# Create a dictionary mapping features to colors\n",
    "feature_color_mapping = {\n",
    "    \"n_sessions\": \"green\",\n",
    "    \"attribution_channel\": \"green\",\n",
    "    \"device_type\": \"green\",\n",
    "    \"time_to_book_week\": \"green\",\n",
    "}\n",
    "\n",
    "# Plot the sorted feature importances with specific colors\n",
    "plt.bar(\n",
    "    range(len(feature_importances_sorted)),\n",
    "    feature_importances_sorted,\n",
    "    tick_label=feature_names_sorted,\n",
    "    color=[\n",
    "        feature_color_mapping.get(feature, \"#87CEEB\")\n",
    "        for feature in feature_names_sorted\n",
    "    ],\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.xticks(rotation=90)  # Set x-axis labels to vertical\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Threshold for new data\n",
    "\n",
    "Based on the test evaluation and desired performance criteria, a threshold is set to determine the model's decision boundary for classifying future data points.\n",
    "And we want to improve the metrics by retraining if it falls below the threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_specificity, test_acc, test_f1 = get_scores(model, X_test, y_test)\n",
    "threshold = test_specificity\n",
    "print(f\"Threshold for new {model.__class__.__name__} model: {threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on new data\n",
    "\n",
    "Predicting on assumed new data which we have already splitted from the original dataset.\n",
    "\n",
    "#### Note:\n",
    "\n",
    "    - Preprocess the input data using the `preprocess_data` function.\n",
    "    - Performs feature selection using the `feature_selection` function.\n",
    "    - Reshapes input data for LSTM models before making predictions.\n",
    "    - For non-LSTM models, uses the `predict` method directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.main.predict import predict\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "predict_new_y = predict(model, X_for_pred)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_for_pred, predict_new_y)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "accuracy = accuracy_score(y_for_pred, predict_new_y)\n",
    "f1 = f1_score(y_for_pred, predict_new_y)\n",
    "\n",
    "print(f\"Specificity: {specificity}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline insight and further improvement\n",
    "\n",
    "`Train`: Different model algorithms are trained on the provided dataset.\n",
    "\n",
    "`Evaluate`: Each model's performance is assessed using the validation set and predetermined metrics like accuracy, precision, recall, and F1-score.\n",
    "\n",
    "`Select`: The model with the best scores on those metrics is chosen for final evaluation.\n",
    "\n",
    "`Test`: The chosen model is evaluated on the previously unseen test set to get final scores on the chosen metrics.\n",
    "\n",
    "`Threshold`: Based on the test evaluation and desired performance criteria, a threshold is set to determine the model's decision boundary for classifying future data points.\n",
    "\n",
    "`Retrain`: If further optimization is required, retraining is performed with adjusted parameters or model selection to improve performance and achieve desired scores on the chosen metrics.\n",
    "This workflow ensures a robust model selection process with accurate performance evaluation and a reliable threshold for future predictions.\n",
    "\n",
    "### Improvement\n",
    "\n",
    "`Train`: Train different model algorithms and hyperparameter tuning to find the best configuration for model using cross validation method and GridSearch.[Unfortunately my PC doesn't allow me to train with that configured model.]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senior_data_scientist_task-lHbaO3K8-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
