{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full pipeline\n",
    "\n",
    "Pipeline includes preprocess the data, feature extraction, train model, evaluate model and predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path_id</th>\n",
       "      <th>device_type</th>\n",
       "      <th>attribution_channel</th>\n",
       "      <th>distance_to_last</th>\n",
       "      <th>time_delta_in_days</th>\n",
       "      <th>has_booked</th>\n",
       "      <th>n_sessions</th>\n",
       "      <th>most_common_landing_page</th>\n",
       "      <th>clicked_city</th>\n",
       "      <th>viewed_city</th>\n",
       "      <th>viewed_beach</th>\n",
       "      <th>saw_brand</th>\n",
       "      <th>saw_organic</th>\n",
       "      <th>saw_direct</th>\n",
       "      <th>saw_offer_summary</th>\n",
       "      <th>saw_panda</th>\n",
       "      <th>adults</th>\n",
       "      <th>children</th>\n",
       "      <th>nights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4c56d801d41290b7b204b55e1565689f</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Search</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4c56d801d41290b7b204b55e1565689f</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Search</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4c56d801d41290b7b204b55e1565689f</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Search</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4c56d801d41290b7b204b55e1565689f</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Search</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4c56d801d41290b7b204b55e1565689f</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Search</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            path_id  device_type  attribution_channel  \\\n",
       "0  4c56d801d41290b7b204b55e1565689f            2                    1   \n",
       "1  4c56d801d41290b7b204b55e1565689f            2                    9   \n",
       "2  4c56d801d41290b7b204b55e1565689f            2                    9   \n",
       "3  4c56d801d41290b7b204b55e1565689f            2                    9   \n",
       "4  4c56d801d41290b7b204b55e1565689f            2                    9   \n",
       "\n",
       "   distance_to_last  time_delta_in_days  has_booked  n_sessions  \\\n",
       "0                 1                   0           1           6   \n",
       "1                 6                   1           1           6   \n",
       "2                 4                   0           1           6   \n",
       "3                 2                   0           1           6   \n",
       "4                 7                   1           1           6   \n",
       "\n",
       "  most_common_landing_page clicked_city viewed_city viewed_beach saw_brand  \\\n",
       "0                   Search        False       False        False     False   \n",
       "1                   Search        False       False        False     False   \n",
       "2                   Search        False       False        False     False   \n",
       "3                   Search        False       False        False     False   \n",
       "4                   Search        False       False        False     False   \n",
       "\n",
       "  saw_organic saw_direct  saw_offer_summary  saw_panda  adults  children  \\\n",
       "0       False       True               True       True     6.0  0.333333   \n",
       "1       False       True               True       True     6.0  0.333333   \n",
       "2       False       True               True       True     6.0  0.333333   \n",
       "3       False       True               True       True     6.0  0.333333   \n",
       "4       False       True               True       True     6.0  0.333333   \n",
       "\n",
       "     nights  \n",
       "0  0.666667  \n",
       "1  0.666667  \n",
       "2  0.666667  \n",
       "3  0.666667  \n",
       "4  0.666667  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from src.main.preprocess_data import merge_data\n",
    "\n",
    "merged_data: DataFrame = merge_data(\n",
    "    \"data/raw/attribution_path_data.feather\", \"data/raw/user_feature_data.feather\"\n",
    ")\n",
    "merged_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device_type</th>\n",
       "      <th>attribution_channel</th>\n",
       "      <th>distance_to_last</th>\n",
       "      <th>time_delta_in_days</th>\n",
       "      <th>has_booked</th>\n",
       "      <th>n_sessions</th>\n",
       "      <th>most_common_landing_page</th>\n",
       "      <th>clicked_city</th>\n",
       "      <th>viewed_city</th>\n",
       "      <th>viewed_beach</th>\n",
       "      <th>saw_brand</th>\n",
       "      <th>saw_organic</th>\n",
       "      <th>saw_direct</th>\n",
       "      <th>saw_offer_summary</th>\n",
       "      <th>saw_panda</th>\n",
       "      <th>adults</th>\n",
       "      <th>children</th>\n",
       "      <th>nights</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>path_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00003d039958362817073f4c9448ff34</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Search</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00004fcd09bcf6bccb16269538032578</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>SEO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000fc42234be9ec2abcbb4c71b4bae4</th>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Homepage</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000148e9e616d1a4f03c92f566f37b39</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel Details</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00014d81cd6599192b2446c4c65a29c0</th>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel Details</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffeb83084cd63bf98f06a9831713ade</th>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>SEO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffecfee29bb3142a438092ee6ffd638</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Homepage</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffedd1c982de9688cc6bd78a8a8a399</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>SEO</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffff36f1ceada40b90f268dd590460ad</th>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Homepage</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffffb7b95695335c8199e5610a31bcaf</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Homepage</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>228101 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  device_type  attribution_channel  \\\n",
       "path_id                                                              \n",
       "00003d039958362817073f4c9448ff34            3                    7   \n",
       "00004fcd09bcf6bccb16269538032578            3                    4   \n",
       "0000fc42234be9ec2abcbb4c71b4bae4            3                   17   \n",
       "000148e9e616d1a4f03c92f566f37b39            3                    7   \n",
       "00014d81cd6599192b2446c4c65a29c0            5                   21   \n",
       "...                                       ...                  ...   \n",
       "fffeb83084cd63bf98f06a9831713ade            5                   21   \n",
       "fffecfee29bb3142a438092ee6ffd638            2                    2   \n",
       "fffedd1c982de9688cc6bd78a8a8a399            3                    9   \n",
       "ffff36f1ceada40b90f268dd590460ad            3                   17   \n",
       "ffffb7b95695335c8199e5610a31bcaf            3                    9   \n",
       "\n",
       "                                  distance_to_last  time_delta_in_days  \\\n",
       "path_id                                                                  \n",
       "00003d039958362817073f4c9448ff34                 2                   1   \n",
       "00004fcd09bcf6bccb16269538032578                 2                   2   \n",
       "0000fc42234be9ec2abcbb4c71b4bae4                 3                   1   \n",
       "000148e9e616d1a4f03c92f566f37b39                 2                  19   \n",
       "00014d81cd6599192b2446c4c65a29c0                 2                   0   \n",
       "...                                            ...                 ...   \n",
       "fffeb83084cd63bf98f06a9831713ade                 2                   0   \n",
       "fffecfee29bb3142a438092ee6ffd638                 4                   4   \n",
       "fffedd1c982de9688cc6bd78a8a8a399                 2                   0   \n",
       "ffff36f1ceada40b90f268dd590460ad                 2                   0   \n",
       "ffffb7b95695335c8199e5610a31bcaf                 3                   0   \n",
       "\n",
       "                                  has_booked  n_sessions  \\\n",
       "path_id                                                    \n",
       "00003d039958362817073f4c9448ff34           0           1   \n",
       "00004fcd09bcf6bccb16269538032578           0           1   \n",
       "0000fc42234be9ec2abcbb4c71b4bae4           0           2   \n",
       "000148e9e616d1a4f03c92f566f37b39           0           1   \n",
       "00014d81cd6599192b2446c4c65a29c0           0           1   \n",
       "...                                      ...         ...   \n",
       "fffeb83084cd63bf98f06a9831713ade           0           1   \n",
       "fffecfee29bb3142a438092ee6ffd638           1           3   \n",
       "fffedd1c982de9688cc6bd78a8a8a399           0           1   \n",
       "ffff36f1ceada40b90f268dd590460ad           0           1   \n",
       "ffffb7b95695335c8199e5610a31bcaf           0           2   \n",
       "\n",
       "                                 most_common_landing_page clicked_city  \\\n",
       "path_id                                                                  \n",
       "00003d039958362817073f4c9448ff34                   Search        False   \n",
       "00004fcd09bcf6bccb16269538032578                      SEO          NaN   \n",
       "0000fc42234be9ec2abcbb4c71b4bae4                 Homepage        False   \n",
       "000148e9e616d1a4f03c92f566f37b39            Hotel Details          NaN   \n",
       "00014d81cd6599192b2446c4c65a29c0            Hotel Details          NaN   \n",
       "...                                                   ...          ...   \n",
       "fffeb83084cd63bf98f06a9831713ade                      SEO          NaN   \n",
       "fffecfee29bb3142a438092ee6ffd638                 Homepage        False   \n",
       "fffedd1c982de9688cc6bd78a8a8a399                      SEO        False   \n",
       "ffff36f1ceada40b90f268dd590460ad                 Homepage        False   \n",
       "ffffb7b95695335c8199e5610a31bcaf                 Homepage        False   \n",
       "\n",
       "                                 viewed_city viewed_beach saw_brand  \\\n",
       "path_id                                                               \n",
       "00003d039958362817073f4c9448ff34       False        False     False   \n",
       "00004fcd09bcf6bccb16269538032578         NaN          NaN     False   \n",
       "0000fc42234be9ec2abcbb4c71b4bae4       False        False      True   \n",
       "000148e9e616d1a4f03c92f566f37b39       False        False     False   \n",
       "00014d81cd6599192b2446c4c65a29c0       False        False     False   \n",
       "...                                      ...          ...       ...   \n",
       "fffeb83084cd63bf98f06a9831713ade         NaN          NaN     False   \n",
       "fffecfee29bb3142a438092ee6ffd638       False        False      True   \n",
       "fffedd1c982de9688cc6bd78a8a8a399         NaN          NaN     False   \n",
       "ffff36f1ceada40b90f268dd590460ad       False        False      True   \n",
       "ffffb7b95695335c8199e5610a31bcaf       False        False     False   \n",
       "\n",
       "                                 saw_organic saw_direct  saw_offer_summary  \\\n",
       "path_id                                                                      \n",
       "00003d039958362817073f4c9448ff34       False      False              False   \n",
       "00004fcd09bcf6bccb16269538032578       False      False              False   \n",
       "0000fc42234be9ec2abcbb4c71b4bae4       False      False               True   \n",
       "000148e9e616d1a4f03c92f566f37b39       False      False              False   \n",
       "00014d81cd6599192b2446c4c65a29c0       False      False              False   \n",
       "...                                      ...        ...                ...   \n",
       "fffeb83084cd63bf98f06a9831713ade        True      False              False   \n",
       "fffecfee29bb3142a438092ee6ffd638       False      False               True   \n",
       "fffedd1c982de9688cc6bd78a8a8a399       False      False              False   \n",
       "ffff36f1ceada40b90f268dd590460ad       False      False              False   \n",
       "ffffb7b95695335c8199e5610a31bcaf       False       True               True   \n",
       "\n",
       "                                  saw_panda  adults  children  nights  \n",
       "path_id                                                                \n",
       "00003d039958362817073f4c9448ff34       True     6.0       0.0     1.0  \n",
       "00004fcd09bcf6bccb16269538032578      False     0.0       0.0     0.0  \n",
       "0000fc42234be9ec2abcbb4c71b4bae4       True     1.0       0.0     1.0  \n",
       "000148e9e616d1a4f03c92f566f37b39       True     2.0       0.0     1.0  \n",
       "00014d81cd6599192b2446c4c65a29c0      False     0.0       0.0     1.0  \n",
       "...                                     ...     ...       ...     ...  \n",
       "fffeb83084cd63bf98f06a9831713ade      False     0.0       0.0     0.0  \n",
       "fffecfee29bb3142a438092ee6ffd638       True     2.0       0.0     1.0  \n",
       "fffedd1c982de9688cc6bd78a8a8a399      False     0.0       0.0     0.0  \n",
       "ffff36f1ceada40b90f268dd590460ad       True     2.0       1.0     1.0  \n",
       "ffffb7b95695335c8199e5610a31bcaf       True     2.0       0.0     1.0  \n",
       "\n",
       "[228101 rows x 18 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group the data by path ID and aggregate the features\n",
    "grouped_data: DataFrame = merged_data.groupby(\"path_id\").agg(\n",
    "    {\n",
    "        \"device_type\": lambda x: x.mode().iloc[0],\n",
    "        \"attribution_channel\": lambda x: x.mode().iloc[0],\n",
    "        \"distance_to_last\": \"max\",\n",
    "        \"time_delta_in_days\": \"max\",\n",
    "        \"has_booked\": \"max\",\n",
    "        \"n_sessions\": \"max\",\n",
    "        \"most_common_landing_page\": lambda x: x.mode().iloc[0],\n",
    "        \"clicked_city\": \"max\",\n",
    "        \"viewed_city\": \"max\",\n",
    "        \"viewed_beach\": \"max\",\n",
    "        \"saw_brand\": \"max\",\n",
    "        \"saw_organic\": \"max\",\n",
    "        \"saw_direct\": \"max\",\n",
    "        \"saw_offer_summary\": \"max\",\n",
    "        \"saw_panda\": \"max\",\n",
    "        \"adults\": \"max\",\n",
    "        \"children\": \"max\",\n",
    "        \"nights\": \"max\",\n",
    "    }\n",
    ")\n",
    "grouped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting some data for prediction purpose.\n",
    "\n",
    "Splitting data for prediction purpose in last step as use this data as new data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from pandas import DataFrame, concat\n",
    "\n",
    "X: DataFrame = grouped_data.drop(\"has_booked\", axis=1)\n",
    "y = grouped_data[\"has_booked\"].astype(int)\n",
    "X_train, X_for_pred, y_train, y_for_pred = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Combine X and y back into a single DataFrame\n",
    "combined_df: DataFrame = concat([X_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data\n",
    "\n",
    "Preprocess data by cleaning, transforming categorical features to numerical values and imputing NA values.\n",
    "\n",
    "#### Note:\n",
    "\n",
    "    - The function modifies the input DataFrame in place and return processed DataFrame.\n",
    "    - If save_path is provided, the preprocessed data will be saved as a Feather file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Preprocessing data -----------\n",
      "Dropping unnecessary features: ['clicked_city', 'viewed_city', 'viewed_beach']\n"
     ]
    }
   ],
   "source": [
    "from src.main.preprocess_data import preprocess_data\n",
    "\n",
    "processed_data: DataFrame = preprocess_data(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "#### Note:\n",
    "\n",
    "    - Adjust the list of features to drop, based on the specific features needed for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Feature selection -----------\n",
      "Dropping unnecessary features: ['time_delta_in_days', 'distance_to_last']\n"
     ]
    }
   ],
   "source": [
    "from src.main.feature_selection import feature_selection, train_val_test_split\n",
    "\n",
    "featured_data: DataFrame = feature_selection(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 182480 entries, 0 to 182479\n",
      "Data columns (total 14 columns):\n",
      " #   Column                    Non-Null Count   Dtype  \n",
      "---  ------                    --------------   -----  \n",
      " 0   device_type               182480 non-null  float64\n",
      " 1   attribution_channel       182480 non-null  float64\n",
      " 2   n_sessions                182480 non-null  float64\n",
      " 3   most_common_landing_page  182480 non-null  float64\n",
      " 4   saw_brand                 182480 non-null  float64\n",
      " 5   saw_organic               182480 non-null  float64\n",
      " 6   saw_direct                182480 non-null  float64\n",
      " 7   saw_offer_summary         182480 non-null  float64\n",
      " 8   saw_panda                 182480 non-null  float64\n",
      " 9   adults                    182480 non-null  float64\n",
      " 10  children                  182480 non-null  float64\n",
      " 11  nights                    182480 non-null  float64\n",
      " 12  has_booked                182480 non-null  float64\n",
      " 13  time_to_book_week         182480 non-null  float64\n",
      "dtypes: float64(14)\n",
      "memory usage: 19.5 MB\n"
     ]
    }
   ],
   "source": [
    "featured_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device_type</th>\n",
       "      <th>attribution_channel</th>\n",
       "      <th>n_sessions</th>\n",
       "      <th>most_common_landing_page</th>\n",
       "      <th>saw_brand</th>\n",
       "      <th>saw_organic</th>\n",
       "      <th>saw_direct</th>\n",
       "      <th>saw_offer_summary</th>\n",
       "      <th>saw_panda</th>\n",
       "      <th>adults</th>\n",
       "      <th>children</th>\n",
       "      <th>nights</th>\n",
       "      <th>has_booked</th>\n",
       "      <th>time_to_book_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.048148</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.855556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.06250</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182475</th>\n",
       "      <td>3.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182476</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182477</th>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182478</th>\n",
       "      <td>3.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.03125</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.122222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182479</th>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.344444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>182480 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        device_type  attribution_channel  n_sessions  \\\n",
       "0               3.0                  9.0         1.0   \n",
       "1               3.0                 20.0         3.0   \n",
       "2               3.0                 18.0        36.0   \n",
       "3               3.0                  9.0         2.0   \n",
       "4               3.0                 21.0         6.0   \n",
       "...             ...                  ...         ...   \n",
       "182475          3.0                 31.0         1.0   \n",
       "182476          2.0                  9.0         4.0   \n",
       "182477          3.0                  9.0         8.0   \n",
       "182478          3.0                 20.0         4.0   \n",
       "182479          3.0                  9.0         7.0   \n",
       "\n",
       "        most_common_landing_page  saw_brand  saw_organic  saw_direct  \\\n",
       "0                           10.0        0.0          0.0         1.0   \n",
       "1                           17.0        0.0          0.0         0.0   \n",
       "2                           17.0        1.0          1.0         1.0   \n",
       "3                           12.0        0.0          0.0         1.0   \n",
       "4                           11.0        1.0          0.0         0.0   \n",
       "...                          ...        ...          ...         ...   \n",
       "182475                      19.0        0.0          1.0         0.0   \n",
       "182476                      12.0        0.0          0.0         1.0   \n",
       "182477                      10.0        0.0          0.0         1.0   \n",
       "182478                      17.0        0.0          1.0         0.0   \n",
       "182479                      16.0        0.0          0.0         1.0   \n",
       "\n",
       "        saw_offer_summary  saw_panda    adults  children    nights  \\\n",
       "0                     0.0        0.0  0.000000   0.00000  0.000000   \n",
       "1                     0.0        1.0  0.088889   0.00000  0.666667   \n",
       "2                     1.0        1.0  0.048148   0.00000  0.555556   \n",
       "3                     0.0        0.0  0.000000   0.00000  0.000000   \n",
       "4                     1.0        1.0  0.088889   0.06250  1.000000   \n",
       "...                   ...        ...       ...       ...       ...   \n",
       "182475                0.0        0.0  0.000000   0.00000  0.000000   \n",
       "182476                0.0        0.0  0.000000   0.00000  0.000000   \n",
       "182477                0.0        0.0  0.000000   0.00000  0.000000   \n",
       "182478                0.0        1.0  0.033333   0.03125  0.250000   \n",
       "182479                0.0        1.0  0.095238   0.00000  0.714286   \n",
       "\n",
       "        has_booked  time_to_book_week  \n",
       "0              0.0           0.000000  \n",
       "1              0.0           0.266667  \n",
       "2              1.0           0.855556  \n",
       "3              0.0           0.500000  \n",
       "4              0.0           0.011111  \n",
       "...            ...                ...  \n",
       "182475         0.0           0.000000  \n",
       "182476         0.0           0.088889  \n",
       "182477         0.0           0.833333  \n",
       "182478         0.0           0.122222  \n",
       "182479         0.0           0.344444  \n",
       "\n",
       "[182480 rows x 14 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featured_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting data for training, validation, and testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_val, y_train, y_test, y_val = train_val_test_split(featured_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "Train the model and save the model.\n",
    "\n",
    "#### Note:\n",
    "\n",
    "    - The function uses the following models: Logistic Regression, Random Forest, XGBoost, LSTM.\n",
    "    - As this is classification task so those basic classification models are chosen for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression with GridSearchCV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest Classifier with GridSearchCV...\n",
      "Training XGBoost with GridSearchCV...\n",
      "Training LSTM with GridSearchCV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 14:50:26.090389: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Max\n",
      "2023-11-29 14:50:26.090415: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2023-11-29 14:50:26.090420: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2023-11-29 14:50:26.090454: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-11-29 14:50:26.090470: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2023-11-29 14:50:27.008254: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3650/3650 [==============================] - ETA: 0s - loss: 0.4887 - accuracy: 0.7718WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "3650/3650 [==============================] - 39s 10ms/step - loss: 0.4887 - accuracy: 0.7718\n",
      "Epoch 2/10\n",
      "3646/3650 [============================>.] - ETA: 0s - loss: 0.4794 - accuracy: 0.7737WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "3650/3650 [==============================] - 38s 10ms/step - loss: 0.4793 - accuracy: 0.7737\n",
      "Epoch 3/10\n",
      "3647/3650 [============================>.] - ETA: 0s - loss: 0.4778 - accuracy: 0.7725WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "3650/3650 [==============================] - 38s 10ms/step - loss: 0.4777 - accuracy: 0.7725\n",
      "Epoch 4/10\n",
      "3647/3650 [============================>.] - ETA: 0s - loss: 0.4757 - accuracy: 0.7750WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "3650/3650 [==============================] - 947s 260ms/step - loss: 0.4757 - accuracy: 0.7749\n",
      "Epoch 5/10\n",
      "3646/3650 [============================>.] - ETA: 0s - loss: 0.4739 - accuracy: 0.7741WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "3650/3650 [==============================] - 38s 10ms/step - loss: 0.4739 - accuracy: 0.7741\n",
      "Epoch 6/10\n",
      "3650/3650 [==============================] - ETA: 0s - loss: 0.4726 - accuracy: 0.7739WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "3650/3650 [==============================] - 38s 10ms/step - loss: 0.4726 - accuracy: 0.7739\n",
      "Epoch 7/10\n",
      "3650/3650 [==============================] - ETA: 0s - loss: 0.4711 - accuracy: 0.7744WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "3650/3650 [==============================] - 947s 259ms/step - loss: 0.4711 - accuracy: 0.7744\n",
      "Epoch 8/10\n",
      "3650/3650 [==============================] - ETA: 0s - loss: 0.4706 - accuracy: 0.7745WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "3650/3650 [==============================] - 38s 10ms/step - loss: 0.4706 - accuracy: 0.7745\n",
      "Epoch 9/10\n",
      "3647/3650 [============================>.] - ETA: 0s - loss: 0.4698 - accuracy: 0.7756WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "3650/3650 [==============================] - 38s 10ms/step - loss: 0.4698 - accuracy: 0.7756\n",
      "Epoch 10/10\n",
      "3650/3650 [==============================] - ETA: 0s - loss: 0.4691 - accuracy: 0.7738WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "3650/3650 [==============================] - 38s 10ms/step - loss: 0.4691 - accuracy: 0.7738\n"
     ]
    }
   ],
   "source": [
    "from src.main.train_model_optimized import train_models\n",
    "\n",
    "train_models(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model\n",
    "\n",
    "Loads the model as per its extension.\n",
    "\n",
    "#### Note:\n",
    "\n",
    "    - Supports loading models saved with joblib (.joblib), pickle (.pkl), and Keras (.keras) formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.main.load_model import load_model_from_file\n",
    "\n",
    "models = [\n",
    "    (\"RandomForest\", \"models/model_rf.joblib\"),\n",
    "    (\"LogisticRegression\", \"models/model_lr.joblib\"),\n",
    "    (\"XGBoost\", \"models/model_xgboost.pkl\"),\n",
    "    (\"LSTM\", \"models/model_lstm.keras\"),\n",
    "]\n",
    "\n",
    "# Load models into a list\n",
    "loaded_models = [load_model_from_file(file_path) for _, file_path in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RandomForestClassifier(class_weight={0: 0.5586237575456084,\n",
      "                                     1: 4.764482702349869},\n",
      "                       n_estimators=200), LogisticRegression(C=1, max_iter=1000, penalty='l1', solver='liblinear'), XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.2, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=50, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...), <keras.src.engine.sequential.Sequential object at 0x3db806a90>]\n"
     ]
    }
   ],
   "source": [
    "print(loaded_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "\n",
    "#### Note:\n",
    "\n",
    "    - Uses binary classification for the evaluation metrics.\n",
    "    - Uses specificity scores for model comparison.\n",
    "    - Returns the best model based on the highest specificity score.\n",
    "\n",
    "SpecificityScore <- Because it will return correctly the true number of people who will not book and we can target them with the marketing strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -------RandomForestClassifier-------\n",
      "Unique Predictions: [0 1]\n",
      "Specificity: 0.9575648017648141\n",
      "Accuracy: 0.8825076720736519\n",
      "F1 Score: 0.30816392384640207\n",
      "\n",
      " -------LogisticRegression-------\n",
      "Unique Predictions: [0 1]\n",
      "Specificity: 0.9891537471658802\n",
      "Accuracy: 0.9024002630425252\n",
      "F1 Score: 0.26737967914438504\n",
      "\n",
      " -------XGBClassifier-------\n",
      "Unique Predictions: [0 1]\n",
      "Specificity: 0.9832097554997242\n",
      "Accuracy: 0.9043456817185445\n",
      "F1 Score: 0.34392031573012594\n",
      "\n",
      " -------Sequential-------\n",
      "Reshape for LSTM\n",
      "1141/1141 [==============================] - 3s 3ms/step\n",
      "Unique Predictions: [0 1]\n",
      "Specificity: 0.7376371101170415\n",
      "Accuracy: 0.7464105655414291\n",
      "F1 Score: 0.40623596586899335\n",
      "{RandomForestClassifier(class_weight={0: 0.5586237575456084,\n",
      "                                     1: 4.764482702349869},\n",
      "                       n_estimators=200): 0.30816392384640207, LogisticRegression(C=1, max_iter=1000, penalty='l1', solver='liblinear'): 0.26737967914438504, XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.2, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=50, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...): 0.34392031573012594, <keras.src.engine.sequential.Sequential object at 0x3db806a90>: 0.40623596586899335}\n",
      "\n",
      " The best model suits the data by highest f1 metrics is: Sequential\n"
     ]
    }
   ],
   "source": [
    "from src.main.evaluate_model import model_selection, get_scores\n",
    "\n",
    "metric = \"f1\"\n",
    "model = model_selection(loaded_models, X_val, y_val, metric)\n",
    "\n",
    "print(\n",
    "    f\"\\n The best model suits the data by highest {metric} metrics is: {model.__class__.__name__}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix for Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_13\" is incompatible with the layer: expected shape=(None, 1, 13), found shape=(None, 13)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/bhaskar.saikia/Downloads/Senior Data Scientist Task/full_pipeline.ipynb Cell 24\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bhaskar.saikia/Downloads/Senior%20Data%20Scientist%20Task/full_pipeline.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bhaskar.saikia/Downloads/Senior%20Data%20Scientist%20Task/full_pipeline.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/bhaskar.saikia/Downloads/Senior%20Data%20Scientist%20Task/full_pipeline.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(X_test)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bhaskar.saikia/Downloads/Senior%20Data%20Scientist%20Task/full_pipeline.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m conf_matrix \u001b[39m=\u001b[39m confusion_matrix(y_test, y_pred)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bhaskar.saikia/Downloads/Senior%20Data%20Scientist%20Task/full_pipeline.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Plot Confusion Matrix\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/hb/flmxc2857579lhrzf1r1t0s40000gn/T/__autograph_generated_filer_k4efqu.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/bhaskar.saikia/Library/Caches/pypoetry/virtualenvs/senior_data_scientist_task-lHbaO3K8-py3.11/lib/python3.11/site-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_13\" is incompatible with the layer: expected shape=(None, 1, 13), found shape=(None, 13)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.heatmap(\n",
    "    conf_matrix,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    cbar=False,\n",
    "    xticklabels=[\"Predicted 0\", \"Predicted 1\"],\n",
    "    yticklabels=[\"Actual 0\", \"Actual 1\"],\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred, normalize=\"all\")\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.heatmap(\n",
    "    conf_matrix,\n",
    "    annot=True,\n",
    "    cmap=\"Blues\",\n",
    "    fmt=\".2%\",\n",
    "    cbar=False,\n",
    "    xticklabels=[\"Predicted 0\", \"Predicted 1\"],\n",
    "    yticklabels=[\"Actual 0\", \"Actual 1\"],\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix (Percentage)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "feature_names = X_train.columns.to_list()\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "\n",
    "feature_names_sorted = np.array(feature_names)[np.argsort(feature_importances)[::-1]]\n",
    "feature_importances_sorted = np.sort(feature_importances)[::-1]\n",
    "\n",
    "# Create a dictionary mapping features to colors\n",
    "feature_color_mapping = {\n",
    "    \"n_sessions\": \"green\",\n",
    "    \"attribution_channel\": \"green\",\n",
    "    \"device_type\": \"green\",\n",
    "    \"time_to_book_week\": \"green\",\n",
    "}\n",
    "\n",
    "# Plot the sorted feature importances with specific colors\n",
    "plt.bar(\n",
    "    range(len(feature_importances_sorted)),\n",
    "    feature_importances_sorted,\n",
    "    tick_label=feature_names_sorted,\n",
    "    color=[\n",
    "        feature_color_mapping.get(feature, \"#87CEEB\")\n",
    "        for feature in feature_names_sorted\n",
    "    ],\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.xticks(rotation=90)  # Set x-axis labels to vertical\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Threshold for new data\n",
    "\n",
    "Based on the test evaluation and desired performance criteria, a threshold is set to determine the model's decision boundary for classifying future data points.\n",
    "And we want to improve the metrics by retraining if it falls below the threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_specificity, test_acc, test_f1 = get_scores(model, X_test, y_test)\n",
    "threshold = test_specificity\n",
    "print(f\"Threshold for new {model.__class__.__name__} model: {threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on new data\n",
    "\n",
    "Predicting on assumed new data which we have already splitted from the original dataset.\n",
    "\n",
    "#### Note:\n",
    "\n",
    "    - Preprocess the input data using the `preprocess_data` function.\n",
    "    - Performs feature selection using the `feature_selection` function.\n",
    "    - Reshapes input data for LSTM models before making predictions.\n",
    "    - For non-LSTM models, uses the `predict` method directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.main.predict import predict\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "predict_new_y = predict(model, X_for_pred)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_for_pred, predict_new_y)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "accuracy = accuracy_score(y_for_pred, predict_new_y)\n",
    "f1 = f1_score(y_for_pred, predict_new_y)\n",
    "\n",
    "print(f\"Specificity: {specificity}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline insight and further improvement\n",
    "\n",
    "`Train`: Different model algorithms are trained on the provided dataset.\n",
    "\n",
    "`Evaluate`: Each model's performance is assessed using the validation set and predetermined metrics like accuracy, precision, recall, and F1-score.\n",
    "\n",
    "`Select`: The model with the best scores on those metrics is chosen for final evaluation.\n",
    "\n",
    "`Test`: The chosen model is evaluated on the previously unseen test set to get final scores on the chosen metrics.\n",
    "\n",
    "`Threshold`: Based on the test evaluation and desired performance criteria, a threshold is set to determine the model's decision boundary for classifying future data points.\n",
    "\n",
    "`Retrain`: If further optimization is required, retraining is performed with adjusted parameters or model selection to improve performance and achieve desired scores on the chosen metrics.\n",
    "This workflow ensures a robust model selection process with accurate performance evaluation and a reliable threshold for future predictions.\n",
    "\n",
    "### Improvement\n",
    "\n",
    "`Train`: Train different model algorithms and hyperparameter tuning to find the best configuration for model using cross validation method and GridSearch.[Unfortunately my PC doesn't allow me to train with that configured model.]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senior_data_scientist_task-lHbaO3K8-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
